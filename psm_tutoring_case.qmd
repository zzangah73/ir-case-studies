---
title: "Program Evaluation with PSM--Sophomore Tutoring Impact"
author: "Jemma Kwon, Ph.D."
format:
  html:
    toc: true
    toc-depth: 3
    embed-resources: true
  docx: default
execute:
  echo: true
  warning: false
  message: false
  cache: true
freeze: auto
---

## 1. Overview

#### **Evaluation Question**

This study investigates whether participation in tutoring is associated with improved academic performance and persistence among sophomore students. Tutoring is a central form of academic support aimed at promoting student learning and retention, but its effectiveness often depends on when and how students engage with it. Examining tutoring at the sophomore level provides a more precise understanding of whether such interventions can help sustain academic momentum beyond the first year.

Specifically, the study asks: ***Do sophomore students who participated in tutoring at least once during Fall 2022 achieve higher term grade point averages (GPAs) and better persistence (Spring 2023 Retention) than those who did not participate in tutoring?*** The analysis compares outcomes of students who received tutoring at least once during the semester with those who did not, controlling for relevant background characteristics. By focusing on this concrete and measurable outcome, the evaluation seeks to determine whether tutoring participation contributes meaningfully to academic success during this pivotal stage of college progression.

#### **Population**

The population for this study includes all degree-seeking undergraduate students who were classified as sophomores in Fall 2022. This group provides the most analytically relevant foundation for assessing the impact of tutoring. T**he sophomore year represents a critical turning point in students’ academic trajectories**: they have moved past the initial transition into college, established academic routines, and begun to make decisions about majors and future goals. Yet it is also the stage when many students reconsider their academic direction or disengage, leading to higher attrition rates compared to later years.

Once students advance to the junior level, they are generally more secure in their academic paths and far more likely to persist to degree completion. Because of this increased stability, differences in academic outcomes become narrower, and the effects of academic interventions like tutoring become harder to detect. Focusing on sophomores, therefore, enables the study **to capture a population where timely academic support can meaningfully influence persistence and performance**. Limiting the analysis to this group enhances both conceptual clarity and policy relevance, targeting students at a moment when intervention is most likely to alter their academic trajectory.

**Treatment Definition**

The treatment examined in this study is participation in tutoring services during Fall 2022, defined as receiving tutoring at least once during the semester. This binary treatment approach emphasizes **exposure rather than intensity**, aligning with the goal of assessing whether any engagement with tutoring—no matter how brief—correlates with measurable academic benefits.

-   **Group = 1 (Treated):** Students who participated in tutoring at least once during the semester.
-   **Group = 0 (Comparison):** Students who did **not** receive any tutoring during the semester but share **similar demographic and academic backgrounds** (e.g., gender, race/ethnicity, financial aid eligibility, and first-generation status).

By constructing these comparable groups, the analysis establishes a counterfactual framework: what the treated students’ academic outcomes might have been had they not participated in tutoring. This definition supports a rigorous matching-based evaluation and provides actionable insight into the overall effectiveness of tutoring as a broad academic support strategy.

#### **Outcomes and Covariates**

The outcome variable for this study is the **Fall 2022 term grade points** and the **Spring 2023 retention**. This measure provides a direct and continuous indicator of achievement within the same academic context in which the tutoring program operated.

Two covariates are included to account for relevant confounding factors that may influence both tutoring participation and academic outcomes:

-   **Baseline academic performance:** The student’s GPA at the beginning of the semester, reflecting prior achievement and academic preparedness.
-   **Academic workload:** Captured either by the number of attempted credits (a fine-grained measure of engagement) or by enrollment status (part-time vs. full-time, a coarser categorical measure).

Including these covariates helps adjust for differences in students’ academic engagement, capacity, and preparedness—factors that may simultaneously affect their likelihood of seeking tutoring and their eventual grade performance.

[**Sensitivity Note**]{.underline} Because some students may earn course credits outside the home institution (e.g., through cross-registration or concurrent enrollment programs), the analysis relies exclusively on the **campus-specific GPA**. This restriction ensures that the grade points reflect only coursework completed at the university during Fall 2022. Limiting the outcome measure to institutional coursework enhances internal validity by aligning the academic performance metric with the context of the tutoring services provided.

[***Primary Estimand***]{.underline} **ATT (Average Treatment Effect on the Treated),** the causal effect of the service

#### **Design Overview**

-   **Propensity score matching (PSM)** to construct a comparison group comparable to treated students on background covariates.

-   **Exact matching** on structural factors (cohort type, a coarser enroll load, sex) to “lock” strata.

-   **Nearest-neighbor 1:1** matching within strata (no replacement), using a **logit** PS of:

    \begin{aligned}
    &\text{cohort} + \text{full\_part} + \text{sex} + \\
    &\text{race} + \text{pell\_eligible} + \text{parent\_education} \\
    \end{aligned}

-   **Post-match diagnostics:** standardized mean differences (SMD) and difference in the initial standing

-   **Statistical Test: paired**

    -   **Further Diagnostic:**
        -   Continuous/approximately continuous (e.g., baseline GPA): Wilcoxon signed-rank.
        -   paired t-test as a sensitivity check if normality is plausible.
        -   Binary covariates: McNemar’s test (diagnostic for marginal homogeneity).
    -   **Continuous Outcome Effect Test:**
        -   [**Primary (unadjusted, paired):**]{.underline} Paired t-test on within-pair differences (or Wilcoxon signed-rank if non-normal differences).
        -   [**Adjusted for residual imbalance:**]{.underline} Linear regression on pair differences with covariates (e.g., initial GPA, attempted units), or Linear mixed model with a random intercept for pair (for 1:1) or subclass (for variable-ratio), including imbalanced covariates as fixed effects.
    -   **Binary Outcome Effect Test:**
        -   [**Primary (unadjusted, paired)**]{.underline}**:** McNemar’s test.
        -   [**Adjusted for residual imbalance (e.g., initial standing, attempted units):**]{.underline} Conditional logistic regression, stratified by matched pair (or subclass for variable-ratio), including the imbalanced covariates.

#### **Data Source and Handling**

The treatment group for this analysis was obtained from the **Student Services team’s operational data**, specifically their roster listing student IDs for the target *year_term*. Unlike the **Summer Course** case, this project did not require creating a temporary table in the data mart to pool the entire sophomore population. Instead, the relevant cohort could be identified directly within the data mart by filtering **degree-seeking undergraduate students classified as sophomores and enrolled in Fall 2022**.

Two data sets were used:

-   **tutoring_sopho_fa22.csv** — contains student IDs (`SID`) for the *treated group* (`group = 1`), representing students who received tutoring during Fall 2022.
-   **sophomores_pool_fa22.csv** — includes the broader pool of sophomores available for matching as potential controls.

These files were merged and processed to construct the analytic data set used for propensity score matching (PSM) and subsequent outcome analyses.

#### **Scope & Limitations**

-   **Ceiling effect:** Second-semester retention among sophomores is typically very high, resulting in few discordant pairs and consequently **limited statistical power for McNemar’s test** in binary outcome analyses.
-   **Matching dependence:** The matching algorithm is **order- and tie-dependent**, meaning that small variations in control selection or random seeds can produce slightly different matched samples.
-   **Data constraints:** Only campus-specific GPA and credit loads were available for adjustment; additional academic or behavioral covariates (e.g., class attendance, tutoring hours) were not captured in the operational data

#### **Planned Extensions**

Although not a primary focus of this workshop, several alternative matching strategies and model-based extensions are planned for subsequent exploration:

-   **Optimal matching:** Globally minimizes the total distance between treated and comparison units. It is not greedy and is **order-independent**, providing a deterministic and often better-balanced match.
-   **Full matching:** Incorporates all available cases by assigning weights to comparison and treated units. This approach tends to produce excellent covariate balance and allows analysis through weighted regression or generalized estimating equations (GEE) rather than McNemar’s test.
-   **Mahalanobis distance within exact strata:** Performs matching on covariates within predefined exact strata (e.g., gender or first-generation status) without relying on propensity scores. This method provides transparency and robustness when covariates are few and directly comparable.

## 2. ETL via SQL

Below we \*\*show\*\* the SQL used to build the pool and next semester or year retention slices.

``` sql
WITH fa22 AS (  -- base cohort: Fall 2022 sophomores
    SELECT
        e.sid,
        e.cohort                           
        e.sex,
        e.full_part
        e.enrstat_desc1                    AS fa22enrolled,
        e.ethnic_desc2                     AS race,
        e.ethnic_urm1                      AS urm,
        e.stu_level_desc1                  AS fa22level,
        e.pell_eligible,
        e.parent_educ_stat,
        e.college_short,
        e.totalgpa                         AS fa22total,
        e.cum_gpa                          AS fa22cum,
        e.term_gpa                         AS fa22term,
        e.term_gpa_aunits                  AS fa22attempted,
        e.term_gpa_eunits                  AS fa22earned
    FROM dm_enr e
    WHERE e.year_term = '20224'
      AND TRIM(e.stu_level_desc1) = 'b.Sophomores'
),

enroll_pivot AS (  -- pivot Spring 2023 / Fall 2023 enrollment status by SID
    SELECT
        sid,
        MAX(CASE WHEN year_term = '20232' THEN enrstat_desc1 END) AS sp23enrolled,
        MAX(CASE WHEN year_term = '20234' THEN enrstat_desc1 END) AS fa23enrolled
    FROM dm_enr
    WHERE year_term IN ('20232', '20234')
    GROUP BY sid
),

sp22_cum AS (  -- prior cumulative GPA at Spring 2022
    SELECT
        sid,
        MAX(cum_gpa) AS sp22_cum
    FROM dm_enr
    WHERE year_term = '20222'
    GROUP BY sid
),

degree_yt AS (  -- latest degree term for undergraduate/post-bacc (deglevel 2,3)
    SELECT sid, grad_year_term
    FROM (
        SELECT
            d.sid,
            d.grad_year_term,
            ROW_NUMBER() OVER (PARTITION BY d.sid ORDER BY d.grad_year_term DESC) AS rn
        FROM dm_deg d
        WHERE d.deglevel IN (2, 3)
    )
    WHERE rn = 1
)

SELECT
    f.sid,
    f.cohort,
    f.sex,
    f.full_part,
    f.fa22enrolled,
    f.race,
    f.urm,
    f.fa22level,
    f.pell_eligible,
    f.parent_educ_stat,
    f.college_short,
    f.fa22total,
    f.fa22cum,
    f.fa22term,
    f.fa22attempted,
    f.fa22earned,
    p.sp23enrolled,
    p.fa23enrolled,
    s.sp22_cum,
    d.grad_year_term
FROM fa22 f
LEFT JOIN enroll_pivot p ON p.sid = f.sid
LEFT JOIN sp22_cum   s ON s.sid = f.sid
LEFT JOIN degree_yt d ON d.sid = f.sid
ORDER BY f.fa22level, f.sid;
```

-   

------------------------------------------------------------------------

## 3. Constructing the Analytic Sample

Now, we have two data sets: the tutoring roster with student_id and `group` = 1 (n = 146) and the pool data for entire sophomore students in the Fall of 2022 (n = 2823).

In this section we merge those two data sets, harmonize IDs, and produce an analysis-ready data frame for PSM and subsequent statistical tests.

``` r
# Demo-only: showing a Windows-style path and workflow
setwd("C:/Users/FAKE/Desktop/Projects/Tutor")
pool <- read.csv("sopho_pool_fa22.csv")
tutored <- read.csv("tutor_sopho_fa22.csv")

# To me convenient: change column names to lower case 
names(pool) <- tolower(names(pool))
dim(pool)
colnames(pool)
names(tutored) <- tolower(names(tutored))
colnames(tutored)
dim(tutored)

library(dplyr)
library(readr)
library(stringr)

# Mark treated (in summer roster)
treated_ids <- tutored$sid

# Attach group on cohort base
pool_flagged <- pool %>%
  mutate(group = if_else(sid %in% treated_ids, 1L, 0L))

# Find tutored-only ids (not in pool at census) for audit
tutored_only <- anti_join(tutored, pool, by = "sid") %>%
  transmute(sid, group = 2L)

final_data <- bind_rows(pool_flagged, tutored_only)

# Sanity checks
stopifnot(!any(is.na(final_data$sid)))

final_data %>% count(group, name = "n_by_group")
```

```         
# group n_by_group
 1     0       2677
 2     1        146
```

After merging and processing the two data sources, the final dataset included **146 students in the treatment group** and **2,677 students in the comparison pool**, calculated as 2,823 total sophomores minus the 146 treated cases.

``` r
# Drop tutored-only if verified not enrolled in Fall
final_data <- final_data %>% filter(group != 2L)

# Assert counts you expect (edit numbers as needed)
stopifnot(sum(final_data$group == 1L) == 146)

write_csv(final_data, "psm_final_data.csv")
```

This process produces an **analysis-ready dataset** for the full cohort, including all background covariates and a `group` indicator: **1 = treated** (Students who participated in tutoring at least once during the semester) and **0 = comparison pool**.

### Data Preparation

The final data-processing step establishes a **clean, defensible foundation** for the analysis.

1.  **Standardizing Categories** All categorical variables are standardized to maintain consistent labeling across tables, plots, and time periods.
2.  **Normalizing Text Fields**\
    String fields are normalized to eliminate unintended discrepancies that can generate redundant levels or missing values.
3.  **Inspecting and Cleaning Numeric Variables** Continuous variables are examined for outliers, extreme skew, or unintended zeros. When appropriate, values are transformed, imputed, or capped to maintain comparability across terms.
4.  **Ensuring Analytical Readiness**\
    Once all fields are standardized and validated, data types are explicitly set (factors vs. numeric), and variable labels are documented. The cleaned dataset represents the final analytical file from which matching and subsequent retention analyses are conducted. This version is fully auditable, allowing others to reproduce or extend the analysis using identical preprocessing logic.

``` r
data <- final_data

colnames(data)
# Exact matching: "cohort" "sex" "full_part"          
# PSM:  "race" "pell_eligible"    "parent_educ_stat" "college_short"
# including numerical variable causes poor performance : "fa22total" "fa22attempted"  
# outcome variables: "fa22term" "sp23enrolled". 

# Variables to audit/transform
vars_cat <- c(
  "cohort","full_part","sex", "race",
  "pell_eligible","parent_educ-stat","college_short"
)

vars_num <- c("fa22total", "fa22attempted", "fa22term")

# one binary outcome variable is "sp23enrolled."

# One place to define canonical factor levels
factor_spec <- list(
  cohort = c("FTF","UGT", "Continuing"),
  full_part = c("Full-time","Part-time"),
  sex = c("F","M","N"),
  race = c("Asian","Black", "Ltnx","White","International","Other"),
  pell_eligible = c("Not Eligible","Pell Eligible"),
  parent_educ_stat = c("College Experience","First Generation", "Unknown"), 
  college_short = c("Ethnic_Education", "Literature_Art", "Business",
                    "Science_Engineering","Health_Social", "University")
)

normalize_chr <- function(x) stringr::str_squish(stringr::str_trim(as.character(x)))

freq_tbl <- function(df, var, stage){
  if (!var %in% names(df)) return(NULL)
  tibble(variable = var, level = normalize_chr(df[[var]])) |>
    count(variable, level, name = "n") |>
    group_by(variable) |>
    mutate(pct = n/sum(n)) |>
    ungroup() |>
    mutate(stage = stage)
}

# 1) BEFORE table (frequencies)
data_before <- data
freq_before <- purrr::map_dfr(vars_cat, ~freq_tbl(data_before, .x, "before"))

# Column-by-column transformation THEN factorization

# Minimal recodes & surgical clean-ups if values drifted
data <- data %>%
  mutate(
    cohort = case_when(
      normalize_chr(cohort) == "1.FTF" ~ "FTF",
      normalize_chr(cohort) == "2.UGT" ~ "UGT",
      is.na(cohort) | normalize_chr(cohort) %in% c("", "NA") ~ "Continuing",
      TRUE ~ cohort
      ),
    full_part = normalize_chr(full_part),
    sex = normalize_chr(sex),
    race = case_when(
      normalize_chr(race) == "2Black" ~ "Black",
      normalize_chr(race) == "3Asian" ~ "Asian",
      normalize_chr(race) == "6Ltnx" ~ "Ltnx",
      normalize_chr(race) == "7White" ~ "White",
      normalize_chr(race) == "8Intl" ~ "International",
      normalize_chr(race) %in% c("9Unknown", "8TwoMore", "5PacIsl", "1AmInd") ~ "Other", 
      TRUE ~ normalize_chr(race)
    ),
    pell_eligible = case_when(
      as.numeric(pell_eligible) == 1 ~ "Pell Eligible",
      as.numeric(pell_eligible) == 0 ~ "Not Eligible",
      TRUE ~ NA_character_
        ),
    parent_educ_stat = case_when(
      str_starts(normalize_chr(parent_educ_stat), "CollegeExperience") ~ "College Experience",
      str_starts(normalize_chr(parent_educ_stat), "First") ~ "First Generation",
      str_starts(normalize_chr(parent_educ_stat), "Unknown") ~ "Unknown",
      TRUE ~ normalize_chr(parent_educ_stat)
    ),
    college_short = case_when(
      str_detect(normalize_chr(college_short), "LCA") ~ "Literature_Art",
      str_detect(normalize_chr(college_short), "ETHS|EDUC") ~ "Ethnic_Education",
      str_detect(normalize_chr(college_short), "BUS") ~ "Business",
      str_detect(normalize_chr(college_short), "SC/EN") ~ "Science_Engineering",
      str_detect(normalize_chr(college_short), "HSS") ~ "Health_Social",
      str_detect(normalize_chr(college_short), "UNIV") ~ "University",
      TRUE ~ normalize_chr(college_short)
    ),
    beginGPA = suppressWarnings(as.numeric(fa22total)),
    fa22Load = suppressWarnings(as.numeric(fa22attempted)), 
    endGPA = suppressWarnings(as.numeric(fa22term))
  )

# Now factorize against the canonical level sets
for (nm in names(factor_spec)) {
  if (nm %in% names(data)) data[[nm]] <- factor(data[[nm]], levels = factor_spec[[nm]])
}

# 3) AFTER table (frequencies)
freq_after <- purrr::map_dfr(vars_cat, ~freq_tbl(data, .x, "after"))

freq_compare <- bind_rows(freq_before, freq_after) %>%
  tidyr::pivot_wider(names_from = stage, values_from = c(n, pct), values_fill = 0) %>%
  arrange(variable, desc(n_after))

# Display
if (requireNamespace("gt", quietly = TRUE)) {
  freq_compare %>%
    group_by(variable) %>%
    arrange(variable, desc(n_after)) %>%
    gt::gt(rowname_col = "level", groupname_col = "variable") %>%
    gt::fmt_percent(dplyr::all_of(c("pct_before","pct_after")), decimals = 1)
} else {
  freq_compare
}
```

![](tutor_data_process.png){width="379"}

All categorical variables were re-coded as expected and verified against their intended group definitions. The next step involves cleaning the numeric variables that will be used in the model, specifically, the GPA at the beginning and end of the semester and the fine-grained course load (i.e., number of units attempted in Fall 2022), to examine their distributions and identify any implausible or outlier values.

``` r
# check if there is any missing values
sapply(data[vars_num], function(x) sum(is.na(x)))
```

```         
beginGPA fa22Load   endGPA
0        0             0 
```

There is no missing values on the three numeric variable.

``` r
# Check skewness of numeric variables
library(e1071)
# Check skewness of original numeric variables
skew_values <- sapply(data[vars_num], skewness, na.rm = TRUE)

# View which variables are highly skewed
print(skew_values[abs(skew_values) > 1])

# Check for NAs and zero values in specific variables before transformation
sapply(data[vars_num], function(x) {
  c(n_missing = sum(is.na(x)), n_zero = sum(x == 0, na.rm = TRUE))
})
```

```         
 beginGPA  fa22Load    endGPA 
-1.499882 -1.062931 -1.170588

          beginGPA fa22Load endGPA
n_missing        0        0      0
n_zero          47       87    208
```

The three target numeric variables are left-skewed, and a log transformation would exaggerate that skew by compressing higher values and stretching lower ones. Accordingly, a square or cube transformation is more appropriate. Before transforming, we address implausible zeros in `fa22Load` (attempted units in Fall 2022): a value of zero indicates the student did not take any courses and should be excluded from modeling. We therefore remove cases with `fa22Load == 0` and document how many observations are dropped from the treated group (`group = 1`) and the untreated pool (`group = 0`).

``` r
# Tally totals by group and how many will be dropped (fa22Load == 0)
totals_by_group <- data %>% count(group, name = "n_total")
zeros_by_group  <- data %>% filter(fa22Load == 0) %>% count(group, name = "n_drop_zero")

drop_summary <- totals_by_group %>%
  left_join(zeros_by_group, by = "group") %>%
  mutate(
    n_drop_zero = coalesce(n_drop_zero, 0L),
    pct_drop    = n_drop_zero / n_total
  )

print(drop_summary) 
#> Shows, for group = 0 and 1, how many rows have fa22Load == 0 and the % of the group

data <- data %>%
  filter(!(fa22Load == 0))

if (nrow(zeros_by_group) > 0) {
  message(
    sprintf(
      "Dropped fa22Load == 0 cases — group=0: %d, group=1: %d",
      zeros_by_group$n_drop_zero[match(0, zeros_by_group$group)] %>% coalesce(0L),
      zeros_by_group$n_drop_zero[match(1, zeros_by_group$group)] %>% coalesce(0L)
    )
  )
} else {
  message("No cases with fa22Load == 0 — data already clean.")
}
```

```         
  group n_total n_drop_zero    pct_drop
1     0    2677          86 0.032125514
2     1     146           1 0.006849315

Dropped fa22Load == 0 cases — group=0: 86, group=1: 1
```

This final data processing dropped one case from the treated group and 86 from the untreated pool for students who did not take any course in the term under study.

``` r
# square transfromations for left-skewness and then, re-check the distribution
data$sq22pre <- (data$beginGPA)^2
data$sq22post <- (data$endGPA)^2
data$sq22load <- (data$fa22Load)^2

sapply(data[, c("sq22pre", "sq22post", "sq22load")], skewness, na.rm = TRUE)
```

```         
    sq22pre    sq22post    sq22load 
-0.44030314 -0.52184543 -0.05086818 
```

The original GPA and load variables were left-skewed (skewness ≈ −1.0 to −1.5). Since a log transformation would exacerbate the skew by stretching low values and compressing the high end, a square transformation was applied instead. This adjustment effectively reduced skewness to approximately −0.05 to −0.5 across variables, producing distributions that are nearly symmetric and more suitable for the propensity score model.

Now, we have analysis-ready data set. Save processed data as the last version of data set and then, have the final check of variables.

``` r
write.csv(data, "psm_analysis_ready.csv", row.names = FALSE)

# Check all variables one more time
vars_cat <- c("cohort","full_part","sex", "race",
              "pell_eligible","parent_educ-stat","college_short")
lapply(vars_cat, function(v) setNames(list(levels(data[[v]])), v))

vars_num <- c("sq22pre", "sq22post", "sq22load")
```

```         
[[1]]
[[1]]$cohort
[1] "FTF"        "UGT"        "Continuing"


[[2]]
[[2]]$full_part
[1] "Full-time" "Part-time"


[[3]]
[[3]]$sex
[1] "F" "M" "N"


[[4]]
[[4]]$race
[1] "Asian"         "Black"         "Ltnx"          "White"         "International"
[6] "Other"        


[[5]]
[[5]]$pell_eligible
[1] "Not Eligible"  "Pell Eligible"


[[6]]
[[6]]$parent_educ_stat
[1] "College Experience" "First Generation"   "Unknown"           


[[7]]
[[7]]$college_short
[1] "Ethnic_Education"    "Literature_Art"      "Business"            "Science_Engineering"
[5] "Health_Social"       "University" 
```

Now, we can see data entries based on our canonical definition and level of factors.

------------------------------------------------------------------------

## 4. Propensity Score Matching

The matching analysis follows four steps:

1.  Check initial imbalance.
2.  Implement matching.
3.  Assess the quality of matching.
4.  Determine evaluative analysis sample and method.

### 4.1 Check initial imbalance

**Goal:** quantify pre-treatment differences to justify matching and choose covariates.

-   Inspect summary tables and standardized mean differences (SMDs).
-   Look for overlap/positivity (treated units have comparable controls).

**`CreateTableOne(...)`** computes group-wise summaries and **standardized mean differences (SMDs)** on the *raw, unmatched* data split by `group`. Our target is SMDs \< 0.1 (rule of thumb), "well balanced."

``` r
library(tableone)
#--- check initial imbalance
balance_vars <- c(
  "cohort", "full_part", "sex", "race",
  "pell_eligible", "parent_educ_stat","college_short", 
  "sq22pre", "sq22post", "sq22load"
)

# Pre-matching balance
tutor_initial <- CreateTableOne(vars = balance_vars, strata = "group",
                                 data = psm_tutor, test = FALSE)
print(tutor_initial, smd = TRUE)
```

```         
                                   Stratified by group
                                    0                    1                    SMD   
  n                                      2591                  145                  
  cohort (%)                                                                   0.222
     FTF                                   73 ( 2.8)            11 ( 7.6)           
     UGT                                  161 ( 6.2)             7 ( 4.8)           
     Continuing                          2357 (91.0)           127 (87.6)           
  full_part = Part-time (%)               335 (12.9)            10 ( 6.9)      0.203
  sex (%)                                                                      0.075
     F                                   1535 (59.2)            85 (58.6)           
     M                                   1049 (40.5)            60 (41.4)           
     N                                      7 ( 0.3)             0 ( 0.0)           
  race (%)                                                                     0.175
     Asian                                648 (25.0)            31 (21.4)           
     Black                                154 ( 5.9)            14 ( 9.7)           
     Ltnx                                1053 (40.6)            56 (38.6)           
     White                                403 (15.6)            25 (17.2)           
     International                        134 ( 5.2)             6 ( 4.1)           
     Other                                199 ( 7.7)            13 ( 9.0)           
  pell_eligible = Pell Eligible (%)      1076 (41.5)            70 (48.3)      0.136
  parent_educ_stat (%)                                                         0.095
     College Experience                  1637 (63.2)            89 (61.4)           
     First Generation                     847 (32.7)            52 (35.9)           
     Unknown                              107 ( 4.1)             4 ( 2.8)           
  college_short (%)                                                            0.310
     Ethnic_Education                      37 ( 1.4)             0 ( 0.0)           
     Literature_Art                       596 (23.0)            25 (17.2)           
     Business                             467 (18.0)            25 (17.2)           
     Science_Engineering                  776 (29.9)            61 (42.1)           
     Health_Social                        585 (22.6)            27 (18.6)           
     University                           130 ( 5.0)             7 ( 4.8)           
  sq22pre (mean (SD))               104109.63 (38721.02) 107209.93 (43501.91)  0.075
  sq22post (mean (SD))                   9.64 (5.09)         10.89 (4.28)      0.266
  sq22load (mean (SD))                 164.34 (80.22)       183.46 (76.27)     0.244
```

**Large / Structural Imbalance: College affiliation (college_short)** – SMD = 0.310

*Students in Science & Engineering are substantially overrepresented in the tutoring group, while those in Literature & Arts and Health & Social Sciences are underrepresented. This represents a structural source of imbalance across colleges.*

**Moderate Imbalance: Cohort type (cohort)** – SMD = 0.222, **Enrollment status** (`full_part`) – SMD = 0.203, **Race/Ethnicity** (`race`) – SMD = 0.175, **Pell eligibility** (`pell_eligible`) – SMD = 0.136, **Post-semester GPA** (`sq22post`) – SMD = 0.266, **Course load** (`sq22load`) – SMD = 0.244

*These variables show modest but meaningful differences between tutoring and non-tutoring students, suggesting that students who sought tutoring were more likely to be full-time, Pell-eligible, from certain racial/ethnic backgrounds, and enrolled in heavier course loads or specific academic colleges.*

**Small / Borderline Imbalance: Parent education** (`parent_educ_stat`) – SMD = 0.095, **Pre-semester GPA** (`sq22pre`) – SMD = 0.075, **Sex** (`sex`) – SMD = 0.075

*Differences are small and within acceptable limits, though the parent education distribution remains slightly uneven.*

**Well Balanced Already:** None of the categorical or continuous covariates demonstrate near-perfect balance, but several (sex, parent education, pre-semester GPA) are effectively balanced even before matching.

**Continuous Variable Check**

-   **Pre-semester GPA** `(sq22pre`) – SMD = 0.075 (balanced)
-   **Post-semester GPA** (`sq22post)` – SMD = 0.266 (moderate imbalance)
-   **Course load** (`sq22load`) – SMD = 0.244 (moderate imbalance)

*The transformed GPA and course-load variables show reasonable improvement in distributional symmetry, but balance still needs refinement via matching.*

Before matching, the largest source of imbalance lies in **college affiliation**, followed by **post-semester GPA** and **course load**. Other demographic and socioeconomic variables show mild-to-moderate imbalance. These patterns indicate that students who received tutoring were not randomly distributed but were more likely to be academically engaged, full-time students from certain colleges—underscoring the necessity of propensity-score matching to equalize these covariates before estimating tutoring effects.

#### Matching setup

Covariates in the **propensity model**: `cohort + full_part + sex + race + pell_eligible + parent_educ_stat + college_short`

**Exact matching (design / structural locks)**

-   We **exact-match** on: `cohort,` `full_part,` and `sex.`

-   This forces treated and comparison units to be compared **within the same strata** for these key variables; within each stratum, the nearest neighbor on the propensity score is chosen.

-   <div>

    **Why this matters**

    -   The **propensity score (PS)** summarizes the likelihood of treatment given all covariates.

    -   **Near-equal PS** implies similar overall confounding risk, but pairs can still differ on an influential covariate.

    -   Exact matching prevents this by **locking** matches within identical profiles for the most important design factors.

    **Other guardrails (beyond the scope of this workshop)**

    -   **Caliper**: set a maximum allowed PS distance; if no control is close enough, the treated unit is left unmatched. This reduces bad matches (poor overlap) at the cost of a smaller sample.

    -   **Mahalanobis within a PS caliper**: keep pairs close on PS **and** close on specific covariates (e.g., GPA).

    -   **Richer PS models**: add nonlinearity (e.g., splines for GPA) and interactions (e.g., `pell_eligible:race_ethnicity`) so PS similarity better reflects the assignment mechanism.

    </div>

Excluding **`sq22pre`** and `sq22load` **from the PS** and addressing differences in it **in the final model**.

-   **Design-first matching.**
    Our approach emphasizes balancing the background and design covariates (e.g., cohort, enrollment status, sex). By matching on these structural features first, we establish treatment–comparison pairs that are comparable on key baseline characteristics. After matching, we then examine whether *sq22pre* and *sq22load* differ between groups.

-   **Clear inference and transparent reporting.**\
    This workflow supports clearer interpretation:

    -   Match on background characteristics only.
    -   Assess post-match differences in *`sq22pre`* and *`sq22load`*.
    -   If imbalance remains, adjust for these variables in the final outcome model—using ANCOVA or linear mixed modeling for continuous outcomes, or conditional logistic regression instead of standard logistic regression for binary outcomes.

    This approach avoids overloading the propensity score model, maintains conceptual clarity, and yields analyses that are both statistically defensible and easy to communicate.

    **Optional tightening without overfitting the PS model.**\
    If additional control of these numeric covariates is desired, we can apply Mahalanobis distance matching within a PS caliper. This provides localized tightening on *`sq22pre`* and *`sq22load`* while still preserving common support and avoiding functional-form assumptions in the propensity score.

> #### Workflow without Additional Guardrails
>
> 1.  Apply exact matching on the key design factors listed above.
> 2.  Validate post-match balance (aiming for SMD \< 0.10 where feasible).
> 3.  If any imbalance remains, address it in the final outcome model or refine the matching approach as appropriate.

### 4-2. Implement matching.

We use **1:1 nearest-neighbor matching without replacement** so each treated unit forms a unique pair with a control. This produces a clean matched dataset for **paired** analyses.

``` r
library(MatchIt)
psm_tutor <- data

ps_formula <- group ~ cohort + full_part + sex + race + pell_eligible + parent_educ_stat + college_short

set.seed(20251103)

matching <- matchit(
  formula  = ps_formula,
  data     = psm_tutor,
  method   = "nearest",
  distance = "logit",   # default
  ratio    = 1,
  exact    = ~ cohort + full_part + sex
  # replace = FALSE   # for paired comparison; default; omitted for simplicity
  # , estimand = "ATT" # default is ATT; include if you want to be explicit
)

matched_data <- match.data(matching)
```

### 4.3. Assess the quality of matching.

#### Global Check

``` r
summary(matching, un = FALSE)
```

-   The table below presents one row per covariate (or per level of multi-category variables) and summarizes key balance diagnostics:

    -   **Means Treated / Means Control**\
        These columns report the average value of each covariate in the matched treatment and control samples.
        -   For binary indicator (0/1) variables, the means represent proportions. For example, a mean of **0.586** for *sexF* (an exact-matched variable) indicates that **58.6%** of students are female in both groups.
        -   For variables balanced through PS-based matching, such as *raceAsian*, the means may differ slightly. For instance, values of **0.214** (treated) and **0.228** (control) show that **21.4%** of the treated group and **22.8%** of the matched control group are Asian. This is a close alignment, especially considering that the pre-matching proportions were **25%** in the tutoring group and **21.4%** in the broader matching pool.
        -   For continuous covariates, the values reflect arithmetic means.

-   **Std. Mean Diff. (SMD)**\
    Difference in group means divided by a pooled SD. Unit-free measure of imbalance.

    -   **Rule of thumb:** \|SMD\| \< **0.10** is *well balanced*; 0.10–0.15 is usually acceptable in practice.
    -   Advantage: not affected by sample size (unlike p-values).

-   **Var. Ratio**\
    Ratio of the control variance to the treated variance for continuous variables.

    -   Target ≈ **1.0**; values between **0.8 and 1.25** are often considered acceptable.
    -   Not shown (or “.”) for indicator variables.

-   **eCDF Mean / eCDF Max**\
    Differences between the empirical CDFs of treated vs control for a variable.

    -   **eCDF Mean:** average absolute difference across the variable’s support.
    -   **eCDF Max:** largest absolute difference (a Kolmogorov-Smirnov–style metric).
    -   Smaller is better; use these to catch imbalances beyond the mean (e.g., tail issues).

-   **Std. Pair Dist.** *(post-match only)*\
    Average (standardized) distance between matched pairs on this covariate.

    -   Smaller implies tighter pairing on that variable.
    -   Useful for spotting variables where pairs are still relatively far apart even if SMD is small.

-   **How to interpret results**

    -   **Multi-level factors** (e.g., `race`) appear as separate indicator rows such as `raceWhite`, `raceAsian,` etc.

    -   Review **SMDs across all levels**—a single large \|SMD\| indicates imbalance for that level.

        -   **Primary target**: aim for **\|SMD\| \< 0.10** for each row (strive for \< 0.05 when feasible).

        -   **Exact-matched variables** should show **identical means** and **SMD = 0** (by construction).

    -   **Variance ratios** (*continuous variables only*): target **0.8–1.25**. It’s normal for **Var. Ratio** to be blank for binary indicators.

    -   **Distance** row summarizes the propensity score scale; after matching, **SMD ≈ 0** indicates treated and control have similar PS distributions.

    -   **eCDF Max / eCDF Mean**: smaller is better; they’re secondary checks for shape differences when SMD looks fine.

    -   In this PSM, covariates are \*\*binary\*\*, so \*\*eCDF Max\*\* and \*\*eCDF Mean\*\* both equal the \*\*absolute difference in proportions\*\* (\|Means Treated − Means Control\|).

> ***Practical note: with binary contrasts, we can focus on the SMD as the primary diagnostic.***

-   **What to do if a row exceeds the target**

    -   If it’s a rare level (tiny cell), small absolute count differences can inflate SMD; consider collapsing levels (if substantively reasonable) or adjusting for that covariate in the outcome model.
    -   If a continuous covariate shows poor balance (SMD or Var. Ratio), consider adjusting for it flexibly (e.g., spline) in the final model.
    -   Keep the matched design (pairs) intact if your analysis plan uses paired methods (e.g., McNemar, conditional logistic regression with `strata(subclass)`).

> In this workshop, we treat these **diagnostics as checks rather than hard pass/fail gates**: we document residual imbalances and adjust for them in the final analysis (e.g., add covariates to the model or use flexible terms), while preserving the matched pairs for interpretable, practitioner-friendly inference.

```         
Call:
matchit(formula = ps_formula, data = psm_tutor, method = "nearest", 
    distance = "logit", exact = ~cohort + full_part + sex, ratio = 1)

Summary of Balance for Matched Data:
                                   Means Treated Means Control Std. Mean Diff. Var. Ratio
distance                                  0.0661        0.0661          0.0017     0.9999
cohortFTF                                 0.0759        0.0759          0.0000          .
cohortUGT                                 0.0483        0.0483          0.0000          .
cohortContinuing                          0.8759        0.8759          0.0000          .
full_partFull-time                        0.9310        0.9310          0.0000          .
full_partPart-time                        0.0690        0.0690          0.0000          .
sexF                                      0.5862        0.5862          0.0000          .
sexM                                      0.4138        0.4138          0.0000          .
sexN                                      0.0000        0.0000          0.0000          .
raceAsian                                 0.2138        0.2276         -0.0336          .
raceBlack                                 0.0966        0.0966          0.0000          .
raceLtnx                                  0.3862        0.3931         -0.0142          .
raceWhite                                 0.1724        0.1724          0.0000          .
raceInternational                         0.0414        0.0345          0.0346          .
raceOther                                 0.0897        0.0759          0.0483          .
pell_eligibleNot Eligible                 0.5172        0.5034          0.0276          .
pell_eligiblePell Eligible                0.4828        0.4966         -0.0276          .
parent_educ_statCollege Experience        0.6138        0.5655          0.0992          .
parent_educ_statFirst Generation          0.3586        0.4000         -0.0863          .
parent_educ_statUnknown                   0.0276        0.0345         -0.0421          .
college_shortEthnic_Education             0.0000        0.0000          0.0000          .
college_shortLiterature_Art               0.1724        0.1724          0.0000          .
college_shortBusiness                     0.1724        0.1862         -0.0365          .
college_shortScience_Engineering          0.4207        0.4207          0.0000          .
college_shortHealth_Social                0.1862        0.1931         -0.0177          .
college_shortUniversity                   0.0483        0.0276          0.0965          .
                                   eCDF Mean eCDF Max Std. Pair Dist.
distance                              0.0006   0.0138          0.0036
cohortFTF                             0.0000   0.0000          0.0000
cohortUGT                             0.0000   0.0000          0.0000
cohortContinuing                      0.0000   0.0000          0.0000
full_partFull-time                    0.0000   0.0000          0.0000
full_partPart-time                    0.0000   0.0000          0.0000
sexF                                  0.0000   0.0000          0.0000
sexM                                  0.0000   0.0000          0.0000
sexN                                  0.0000   0.0000          0.0000
raceAsian                             0.0138   0.0138          0.0336
raceBlack                             0.0000   0.0000          0.0138
raceLtnx                              0.0069   0.0069          0.0708
raceWhite                             0.0000   0.0000          0.0138
raceInternational                     0.0069   0.0069          0.1039
raceOther                             0.0138   0.0138          0.0483
pell_eligibleNot Eligible             0.0138   0.0138          0.0828
pell_eligiblePell Eligible            0.0138   0.0138          0.0828
parent_educ_statCollege Experience    0.0483   0.0483          0.0992
parent_educ_statFirst Generation      0.0414   0.0414          0.0863
parent_educ_statUnknown               0.0069   0.0069          0.0421
college_shortEthnic_Education         0.0000   0.0000          0.0000
college_shortLiterature_Art           0.0000   0.0000          0.0138
college_shortBusiness                 0.0138   0.0138          0.0730
college_shortScience_Engineering      0.0000   0.0000          0.0138
college_shortHealth_Social            0.0069   0.0069          0.0886
college_shortUniversity               0.0207   0.0207          0.0965

Sample Sizes:
          Control Treated
All          2591     145
Matched       145     145
Unmatched    2446       0
Discarded       0       0
```

The matching procedure produced an excellent balance between the treatment and control groups across all observed covariates. The **propensity score (distance)** achieved nearly identical means (0.0661 for both groups) with a negligible standardized mean difference (SMD = 0.0017) and a variance ratio of 0.9999, confirming that the matching effectively aligned the two groups on their estimated likelihood of treatment.

Demographic and academic composition were also well balanced.

-   Exact matching variables: perfect alignment (SMD = 0).
-   Racial and ethnic distributions were very similar, with the largest standardized differences being minimal—**race Other (SMD = 0.048)** and **race Asian (SMD = –0.034)**—well below the conventional 0.1 threshold for acceptable balance.
-   Financial aid and parental education indicators improved substantially after matching: **Pell eligibility** differences reduced to around 0.028, and **first-generation status** differences dropped to –0.086.
-   College affiliation variables were also well aligned, with the largest deviation observed for **University College (SMD = 0.097)** but still within acceptable range.

Distributional checks using **empirical cumulative distribution function (eCDF)** metrics showed minimal differences (all ≤ 0.05 for eCDF mean and ≤ 0.02 for eCDF max), indicating strong overall alignment across the entire distribution of each variable, not just the means.

In summary, the **matching performance is excellent**, with all standardized mean differences well below 0.1 and variance ratios close to 1. This confirms that the treatment and control groups are statistically equivalent on observed characteristics, ensuring that subsequent outcome analyses are unlikely to be biased by baseline imbalances.

#### Check for Other Numeric Covariates

Now, we examine post matching differences in GPA at the beginning of the fall 22 and the units attempted between treated students and their matched counterparts. In this process, two key variables are used:

-   **`group`**: Created during data processing; coded as 1 for students in the treated group (those who took the course) and 0 for students in the control group.
-   **`subclass`**: Generated automatically during the propensity score matching procedure; it identifies the matched pairs (or subclasses) of students.

***1) Balance metric***

``` r
vars <- c("sq22pre", "sq22load")
post_bal <- CreateTableOne(vars = vars, strata = "group",
                           data = as.data.frame(matched_data), test = FALSE)
print(post_bal, smd = TRUE)
```

```         
Stratified by group
                             0                   1                    SMD   
  n                         145                  145                  
  sq22pre (mean (SD))  98221.23 (39809.52) 107209.93 (43501.91)      0.216
  sq22load (mean (SD))   176.63 (78.50)       183.46 (76.27)         0.088
```

> The SMD for attempted credits indicates improved balance between the treated and matched control groups, whereas beginning GPA shows substantial imbalance favoring the treated group. Therefore, beginning GPA should be included as a covariate in the final model.
>
> We can verify these patterns of balance and imbalance by examining the variance ratios as an additional diagnostic check.

``` r
vr <- matched_data %>%
  summarise(across(
    all_of(vars),
    list(
      var_t = ~ var(.x[group == 1], na.rm = TRUE),
      var_c = ~ var(.x[group == 0], na.rm = TRUE),
      var_ratio = ~ var(.x[group == 1], na.rm = TRUE) / var(.x[group == 0], na.rm = TRUE)
    ),
    .names = "{.col}_{.fn}"
  ))

print(vr) # Rule of thumb: Ratios between 0.8 and 1.25 are usually acceptable.
```

```         
     sq22pre_var_t  sq22pre_var_c   sq22pre_var_ratio  sq22load_var_t   sq22load_var_c   sq22load_var_ratio
1    1892416217     1584798095      1.194106           5817.084         6162.956          0.9438789
```

> The results confirm what the SMD indicated. The variance ratio for attempted credits is very close to 1.0 (≈0.94), showing that the treated and control groups have nearly identical variability after matching. In contrast, the variance ratio for beginning GPA is 1.17, suggesting some remaining imbalance in that covariate.

***2) Paired tests on within-pair differences***

-   To evaluate differences in the beginning GPA within matched pairs, we explicitly created a new variable, **`diffs`**, representing the pairwise difference (*treated − control*). This allows us to test whether the median difference (Wilcoxon signed-rank test) or the mean difference (paired t-test) is statistically equal to zero.
-   A **histogram of `diffs`** provides a quick view of the distribution of these differences, while a **QQ plot (`qqnorm(diffs); qqline(diffs)`)** is the standard diagnostic for assessing approximate normality. The normality check is relevant only for the paired t-test, since the Wilcoxon signed-rank test does not require this assumption.
-   For completeness, we implemented both approaches: the Wilcoxon test as the more robust method, and the paired t-test as a sensitivity check.

``` r
paired_is <- matched_data %>%
  filter(!is.na(sq22pre)) %>%
  group_by(subclass) %>%
  filter(n() == 2) %>%
  ungroup() %>%
  mutate(trt_label = ifelse(group == 1, "treated", "control")) %>%
  select(subclass, trt_label, sq22pre) %>%
  pivot_wider(names_from = trt_label, values_from = sq22pre) %>%
  tidyr::drop_na(treated, control)

# Differences (treated − control)
diffs <- paired_is$treated - paired_is$control

# Quick glance & tests
summary(diffs)
hist(diffs, breaks = 30)
qqnorm(diffs)
qqline(diffs)
wilcox.test(diffs, mu = 0, exact = FALSE) 
t.test(paired_is$treated, paired_is$control, paired = TRUE)   # sensitivity
```

![](tutoringHistogram.png){width="257"}

![](tutoringqqline.png){width="270"}

> The distribution shows clear skew, heavy tails, and outliers, indicating it is not consistent with normality. Therefore, the Wilcoxon signed-rank test was used, with a paired t-test included as a sensitivity check.

```         
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 -94932  -17273   15225    8989   37544  160000 

    
	Wilcoxon signed rank test with continuity correction

data:  diffs
V = 5985.5, p-value = 0.006853
alternative hypothesis: true location is not equal to 0

	Paired t-test

data:  paired_is$treated and paired_is$control
t = 2.5711, df = 144, p-value = 0.01115
alternative hypothesis: true mean difference is not equal to 0
95 percent confidence interval:
  2078.409 15898.984
  
sample estimates:
mean difference 
       8988.697 
```

-   **Wilcoxon signed-rank test:** V = 5985.5, *p* = 0.006853
-   **Paired t-test:** *t* = 2.5711, df = 218, *p* = 0.01115; mean difference = 8,988.7; 95% CI \[2,078.4, 15,899.0\]
-   Both tests indicate a significant within-pair difference between the treated and matched control groups.

> **Baseline GPA balance.** After matching, `sq22pre` still showed imbalance (SMD = 0.22). Using complete pairs (n = 145), both the Wilcoxon signed-rank test and the paired t-test confirmed a statistically significant shift between the groups. Therefore, beginning GPA should be included as a covariate in the final outcome model so that the model estimates the program’s impact on end-of-semester GPA or subsequent-semester retention net of baseline academic differences.

------------------------------------------------------------------------

## 5. Final Modeling to Test Impact of the Program

### 5-1. Impact on the Post GPA

To evaluate program effects on subsequent term GPA while accounting for matched-pair dependence, we fitted a linear mixed-effects model with subclass included as a random intercept. Because both the pre- and post-GPA variables were square-transformed in the original dataset, each was standardized prior to modeling to improve numerical stability.

``` r
library(tidyr)

df_sopho <- matched_data
colnames(df_sopho)

# Convert group labels for clarity
df_sopho <- df_sopho %>%
  mutate(group = ifelse(group == 1, "tutoring", "comparison"))

# Rescale: to improve numerical stability without changing statistical significance.
df_sopho$sq22pre_z  <- scale(df_sopho$sq22pre)
df_sopho$sq22post_z <- scale(df_sopho$sq22post)

# ------- post term GPA while preGPA controlled

library(lme4)
# linear mixed modeling with subclass as a random effect 
# to account for dependence within matched pairs

library(lmerTest)
# calculating p-values in mixed models is tricky due to uncertainty in estimating degrees of freedom.
# lmerTest gives p-values calculated using Satterthwaite's approximation for degrees of freedom.

model_gpa <- lmer(sq22post_z ~ group + sq22pre_z + (1 | subclass), data = df_sopho)
summary(model_gpa)
```

```         
Linear mixed model fit by REML. t-tests use Satterthwaite's method ['lmerModLmerTest']
Formula: sq22post_z ~ group + sq22pre_z + (1 | subclass)
   Data: df_sopho

REML criterion at convergence: 778.5

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.2122 -0.6179  0.1486  0.6166  2.1903 

Random effects:
 Groups   Name        Variance Std.Dev.
 subclass (Intercept) 0.1774   0.4212  
 Residual             0.6770   0.8228  
Number of obs: 290, groups:  subclass, 145

Fixed effects:
               Estimate Std. Error        df t value Pr(>|t|)    
(Intercept)    -0.13185    0.07700 274.80400  -1.712  0.08797 .  
grouptutoring   0.26370    0.09739 145.72846   2.708  0.00759 ** 
sq22pre_z       0.36838    0.05645 260.67471   6.526  3.5e-10 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Correlation of Fixed Effects:
            (Intr) grpttr
grouptutrng -0.632       
sq22pre_z    0.079 -0.124
```

The random-effects structure indicated meaningful within-pair dependence (τ = 0.42), supporting the use of a mixed model. After controlling for baseline GPA, students in the tutoring group had significantly higher post-term GPA scores than their matched counterparts. Specifically, the tutoring group showed a **0.26-SD increase** in post-GPA compared with the matched comparison group (*β* = 0.264, *SE* = 0.097, *df* ≈ 146, *p* = 0.0076). Baseline GPA remained a strong positive predictor of post-GPA (*β* = 0.368, *SE* = 0.056, *p* \< 0.001).

Together, these findings indicate that the tutoring program was associated with modest but statistically significant gains in subsequent term academic performance after adjustment for baseline standing and the matched-pair structure.

#### Interpretable Results

To enhance interpretability, the estimated marginal means were back-transformed to the original GPA scale.

``` r
library(emmeans)
emm <- emmeans(model_gpa, ~ group)
emm

# mean of the original sq22post before scaling
m  <- attr(df_sopho$sq22post_z, "scaled:center")
sd <- attr(df_sopho$sq22post_z, "scaled:scale")

# emmeans output (example)
emm_values <- summary(emm)$emmean   # these are standardized

# convert back to squared GPA
sq_post_pred_sq <- (emm_values * sd + m)

# back to GPA
raw_gpa <- sqrt(sq_post_pred_sq)
raw_gpa
```

```         
 group      emmean    SE  df lower.CL upper.CL
 comparison -0.132 0.077 275  -0.2834   0.0197
 tutoring    0.132 0.077 275  -0.0197   0.2834

Degrees-of-freedom method: kenward-roger 
Confidence level used: 0.95 

[1] 3.077854 3.272435
```

The difference is: 0.132 − (−0.132) = 0.264 SD units. This exactly matches the fixed-effect coefficient from your mixed model (`β = 0.2637`).

The predicted post-term GPA was **3.27** for the tutoring group and **3.08** for the matched comparison group, reflecting an average improvement of approximately **+0.19 GPA points** attributable to participation in tutoring.

These results suggest that the tutoring program has a modest but meaningful positive effect on subsequent academic performance after adjusting for initial academic standing.

### 5-2. Impact on the Subsequent Year Retention

To estimate the *total* effect of the tutoring program on subsequent-year retention, a conditional logistic regression model was fitted with matched-pair subclasses as strata and baseline GPA (standardized) included as a covariate. Post-GPA was intentionally excluded to avoid over-adjustment bias, as it represents a potential mediator of the tutoring effect rather than a pre-treatment confounder.

> ***What about using the Post GPA as a covariate?***
>
> If the tutoring program improves GPA, and GPA subsequently influences retention, then post-GPA functions as a *mediator* rather than a baseline confounder. Including a mediator in the model “blocks” part of the tutoring effect you are trying to estimate—a form of over-adjustment bias. In other words, adjusting for post-GPA would artificially remove the portion of the program’s impact that operates through improved academic performance.

In institutional research practice, retention is typically defined as *enrolled in the subsequent fall term* **or** *graduated before that fall*. This means students who graduate in the spring or summer immediately preceding the next fall are considered retained.

``` r
# Ensure outcome is coded binary at the individual level
table(df_sopho$fa23enrolled, df_sopho$group)

df_sopho <- df_sopho %>%
  mutate(
    fa23binary = case_when(
      str_detect(fa23enrolled, "^1\\.Continuing$") ~ 1,    # Enrolled in Fall 23
      grad_year_term %in% c(20232, 20234) ~ 1,            # Graduated by Spring/Fall 23
      TRUE ~ 0                                            # Otherwise not retained
    )
  )

table(df_sopho$fa23binary, df_sopho$group)

# Unadjusted retention rates
df_sopho %>%
  group_by(group) %>%
  summarize(
    retention_rate = mean(fa23binary, na.rm = TRUE),
    n = n()
  )
```

```         
               comparison tutoring
                       18       21
  1.Continuing        127      124
  
      comparison tutoring
  0         18       21
  1        127      124
  
    group      retention_rate     n
  <chr>               <dbl> <int>
1 comparison          0.876   145
2 tutoring            0.855   145
```

The original **`fa23enrolled`** column contains the string value `"1.Continuing"` for students who enrolled in Fall 2023, and missing values indicate non-enrollment. Accordingly, a new binary variable **`fa23binary`** was created, with **1 = enrolled** and **0 = not enrolled**. Both groups' descriptive (unadjusted) retention rates are higher, 85.5% for the tutoring group and 87.6% for the matched comparison group, than the boarder pool (pre-matched pool, 83%).

The key checkpoints for the final retention model are as follows:

-   **Conditional logistic regression** is the appropriate method for analyzing retention outcomes in matched data.
-   **Baseline GPA is** included as a covariate, allowing estimation of the *net* impact of the tutoring program on subsequent-year retention without over-adjustment bias.
-   The use of **strata(subclass)** correctly incorporates the PSM subclass structure and accounts for the matched-pair dependence.

``` r
library(survival)

model_retention <- clogit(fa23binary ~ group + sq22pre_z + strata(subclass), data = df_sopho)
summary(model_retention)
exp(confint(model_retention))

library(broom)
tidy(model_retention, exponentiate = TRUE, conf.int = TRUE)

```

```         
Call:
coxph(formula = Surv(rep(1, 290L), fa23binary) ~ group + sq22pre_z + 
    strata(subclass), data = df_sopho, method = "exact")

  n= 290, number of events= 251 

                  coef exp(coef) se(coef)      z Pr(>|z|)
grouptutoring -0.04548   0.95554  0.40347 -0.113     0.91
sq22pre_z     -0.23838   0.78791  0.40923 -0.582     0.56

              exp(coef) exp(-coef) lower .95 upper .95
grouptutoring    0.9555      1.047    0.4333     2.107
sq22pre_z        0.7879      1.269    0.3533     1.757

Concordance= 0.543  (se = 0.119 )
Likelihood ratio test= 0.6  on 2 df,   p=0.7
Wald test            = 0.59  on 2 df,   p=0.7
Score (logrank) test = 0.6  on 2 df,   p=0.7


                  2.5 %   97.5 %
grouptutoring 0.4333262 2.107087
sq22pre_z     0.3532939 1.757168


# A tibble: 2 × 7
  term          estimate std.error statistic p.value conf.low conf.high
  <chr>            <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl>
1 grouptutoring    0.956     0.403    -0.113   0.910    0.433      2.11
2 sq22pre_z        0.788     0.409    -0.582   0.560    0.353      1.76
```

Across the 290 matched students, 251 were retained. After adjustment for baseline GPA and the matched design, **tutoring participation was not significantly associated with Fall 2023 enrollment**. The odds ratio for the tutoring group was **0.96** (OR = 0.96, 95% CI: 0.43–2.11; *p* = 0.91), indicating no detectable increase or decrease in retention relative to matched controls.

Baseline GPA also showed no significant association with subsequent-year retention (OR = 0.79, 95% CI: 0.35–1.76; *p* = 0.56). These findings suggest that, within this matched cohort, neither tutoring participation nor baseline academic standing predicted retention into the following academic year.

## 6. Conclusion

This evaluation found that tutoring participation was associated with a measurable improvement in academic performance. After matching and adjusting for baseline differences, tutoring students achieved post-term GPAs that were approximately **0.19 points higher** than those of comparable non-participants—a modest but meaningful academic benefit.

However, these GPA gains did **not** translate into detectable differences in subsequent-year retention. Several factors help explain this pattern. First, propensity score matching removes students who cannot be suitably matched to tutoring participants—often those with lower academic preparation, inconsistent enrollment behavior, or elevated risk profiles. As a result, the matched comparison group becomes more retention-stable, with a retention rate of **87.6%**, compared with **83%** in the full pool. This higher and more homogeneous retention baseline makes additional program-related differences harder to detect.

Second, by the sophomore-to-junior transition, **non-academic factors** tend to dominate retention outcomes. Financial constraints, advising access, sense of belonging, major fit, life circumstances, and family responsibilities all play important roles and fall largely outside the influence of an academic support program such as tutoring. Thus, even meaningful improvements in GPA may have limited leverage on retention at this stage.

From a methodological perspective, propensity score matching is highly appropriate for evaluating the tutoring program’s academic effects. However, for studies focused specifically on **retention drivers**, alternative analytical approaches may be more appropriate. Because matching narrows the analytic sample to students who are more similar on key risk factors, the resulting reduction in heterogeneity raises overall retention rates and limits the variation needed to identify predictors of persistence.

Overall, these findings highlight a nuanced reality: tutoring contributes positively to academic performance, but improvements in GPA alone may not be sufficient to shift retention outcomes, especially within an already academically stable and high-retention matched cohort.
